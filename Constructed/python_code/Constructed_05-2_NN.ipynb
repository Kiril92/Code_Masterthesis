{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28f5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f9d5db86fc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import random\n",
    "import math\n",
    "import dill\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import device\n",
    "\n",
    "# Tensorflow soll auf CPU und nicht auf der GPU laufen\n",
    "device(\"cpu:0\")\n",
    "# für GPU:\n",
    "# tf.device(\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4cfebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/home/jovyan/rna/_functions/functions.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b356f410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 08:10:02.781769: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-03 08:10:02.781800: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3c67a",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cb3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_data_combined = \"/home/jovyan/rna/Constructed/data/combined_data/constructed_images_combined_60_2022_08_25.csv\"\n",
    "filepath_labels = \"/home/jovyan/rna/Constructed/data/combined_data/constructed_labels_60_2022_08_25.csv\"\n",
    "\n",
    "X = np.genfromtxt(filepath_data_combined, delimiter = \",\")\n",
    "y = np.array(pd.read_csv(filepath_labels, header=None))\n",
    "\n",
    "# Factorize into dichotomous variable\n",
    "Y = pd.factorize(y[:, 0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d954d8",
   "metadata": {},
   "source": [
    "### Load Persistences Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_folderpath = \"/home/jovyan/rna/Constructed/data/persistence_landscapes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6f815523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Eingelesenen Dateien\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledWithin*.pkl\")[-1])\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledWithin*.pkl\")[-1])\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledBetween*.pkl\")[-1])\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledBetween*.pkl\")[-1])\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_unscaled*.pkl\")[-1])\n",
    "# print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_unscaled*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"PL_H0_scaledWithin*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"PL_H1_scaledWithin*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"PL_H0_scaledBetween*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"PL_H1_scaledBetween*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_unscaled = load_file(file = glob.glob(pl_folderpath + \"PL_H0_unscaled*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_unscaled = load_file(file = glob.glob(pl_folderpath + \"PL_H1_unscaled*.pkl\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989417d2",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f34a2",
   "metadata": {},
   "source": [
    "# Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba0f342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folderpath to store the results\n",
    "folderpath_results = \"/home/jovyan/rna/Constructed/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a06ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for persistence landscapes\n",
    "pl_resolution = 200\n",
    "pl_num_landscapes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "image_vector_size = 6*6\n",
    "\n",
    "# Transform into categorical variable\n",
    "Y_categorical = to_categorical(Y)\n",
    "\n",
    "seed(999)\n",
    "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X.reshape(len(X), image_vector_size),\n",
    "                                                                    np.asarray(Y),\n",
    "                                                                    test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca129442",
   "metadata": {},
   "source": [
    "# Neural Networks on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18cf0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_NN_rawdata_model_1():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=[36,]))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_rawdata_model_2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, activation='relu', input_shape=[36,]))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_rawdata_model_3():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, activation='relu', input_shape=[36,]))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a8befd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for gridsearch\n",
    "simple_NN_rawdata_model = [simple_NN_rawdata_model_1, simple_NN_rawdata_model_2, simple_NN_rawdata_model_3]\n",
    "simple_NN_rawdata_learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "simple_NN_rawdata_validation_splits = [0.33, None]\n",
    "simple_NN_rawdata_epochs = [3, 4, 5, 8, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c6e1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_simple_NN_rawdata = create_modelgrid(models = simple_NN_rawdata_model,\n",
    "                                          learning_rates = simple_NN_rawdata_learning_rates,\n",
    "                                          validation_splits = simple_NN_rawdata_validation_splits,\n",
    "                                          epochs = simple_NN_rawdata_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9f03bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 180\n",
      "Training model 2 of 180\n",
      "Training model 3 of 180\n",
      "Training model 4 of 180\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f9b981e4c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9b6c4cfb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9b9029fb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training model 5 of 180\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f9b6c4cfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f9b981d0c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9b6c6e93a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training model 6 of 180\n",
      "Training model 7 of 180\n",
      "Training model 8 of 180\n",
      "Training model 9 of 180\n",
      "Training model 10 of 180\n",
      "Training model 11 of 180\n",
      "Training model 12 of 180\n",
      "Training model 13 of 180\n",
      "Training model 14 of 180\n",
      "Training model 15 of 180\n",
      "Training model 16 of 180\n",
      "Training model 17 of 180\n",
      "Training model 18 of 180\n",
      "Training model 19 of 180\n",
      "Training model 20 of 180\n",
      "Training model 21 of 180\n",
      "Training model 22 of 180\n",
      "Training model 23 of 180\n",
      "Training model 24 of 180\n",
      "Training model 25 of 180\n",
      "Training model 26 of 180\n",
      "Training model 27 of 180\n",
      "Training model 28 of 180\n",
      "Training model 29 of 180\n",
      "Training model 30 of 180\n",
      "Training model 31 of 180\n",
      "Training model 32 of 180\n",
      "Training model 33 of 180\n",
      "Training model 34 of 180\n",
      "Training model 35 of 180\n",
      "Training model 36 of 180\n",
      "Training model 37 of 180\n",
      "Training model 38 of 180\n",
      "Training model 39 of 180\n",
      "Training model 40 of 180\n",
      "Training model 41 of 180\n",
      "Training model 42 of 180\n",
      "Training model 43 of 180\n",
      "Training model 44 of 180\n",
      "Training model 45 of 180\n",
      "Training model 46 of 180\n",
      "Training model 47 of 180\n",
      "Training model 48 of 180\n",
      "Training model 49 of 180\n",
      "Training model 50 of 180\n",
      "Training model 51 of 180\n",
      "Training model 52 of 180\n",
      "Training model 53 of 180\n",
      "Training model 54 of 180\n",
      "Training model 55 of 180\n",
      "Training model 56 of 180\n",
      "Training model 57 of 180\n",
      "Training model 58 of 180\n",
      "Training model 59 of 180\n",
      "Training model 60 of 180\n",
      "Training model 61 of 180\n",
      "Training model 62 of 180\n",
      "Training model 63 of 180\n",
      "Training model 64 of 180\n",
      "Training model 65 of 180\n",
      "Training model 66 of 180\n",
      "Training model 67 of 180\n",
      "Training model 68 of 180\n",
      "Training model 69 of 180\n",
      "Training model 70 of 180\n",
      "Training model 71 of 180\n",
      "Training model 72 of 180\n",
      "Training model 73 of 180\n",
      "Training model 74 of 180\n",
      "Training model 75 of 180\n",
      "Training model 76 of 180\n",
      "Training model 77 of 180\n",
      "Training model 78 of 180\n",
      "Training model 79 of 180\n",
      "Training model 80 of 180\n",
      "Training model 81 of 180\n",
      "Training model 82 of 180\n",
      "Training model 83 of 180\n",
      "Training model 84 of 180\n",
      "Training model 85 of 180\n",
      "Training model 86 of 180\n",
      "Training model 87 of 180\n",
      "Training model 88 of 180\n",
      "Training model 89 of 180\n",
      "Training model 90 of 180\n",
      "Training model 91 of 180\n",
      "Training model 92 of 180\n",
      "Training model 93 of 180\n",
      "Training model 94 of 180\n",
      "Training model 95 of 180\n",
      "Training model 96 of 180\n",
      "Training model 97 of 180\n",
      "Training model 98 of 180\n",
      "Training model 99 of 180\n",
      "Training model 100 of 180\n",
      "Training model 101 of 180\n",
      "Training model 102 of 180\n",
      "Training model 103 of 180\n",
      "Training model 104 of 180\n",
      "Training model 105 of 180\n",
      "Training model 106 of 180\n",
      "Training model 107 of 180\n",
      "Training model 108 of 180\n",
      "Training model 109 of 180\n",
      "Training model 110 of 180\n",
      "Training model 111 of 180\n",
      "Training model 112 of 180\n",
      "Training model 113 of 180\n",
      "Training model 114 of 180\n",
      "Training model 115 of 180\n",
      "Training model 116 of 180\n",
      "Training model 117 of 180\n",
      "Training model 118 of 180\n",
      "Training model 119 of 180\n",
      "Training model 120 of 180\n",
      "Training model 121 of 180\n",
      "Training model 122 of 180\n",
      "Training model 123 of 180\n",
      "Training model 124 of 180\n",
      "Training model 125 of 180\n",
      "Training model 126 of 180\n",
      "Training model 127 of 180\n",
      "Training model 128 of 180\n",
      "Training model 129 of 180\n",
      "Training model 130 of 180\n",
      "Training model 131 of 180\n",
      "Training model 132 of 180\n",
      "Training model 133 of 180\n",
      "Training model 134 of 180\n",
      "Training model 135 of 180\n",
      "Training model 136 of 180\n",
      "Training model 137 of 180\n",
      "Training model 138 of 180\n",
      "Training model 139 of 180\n",
      "Training model 140 of 180\n",
      "Training model 141 of 180\n",
      "Training model 142 of 180\n",
      "Training model 143 of 180\n",
      "Training model 144 of 180\n",
      "Training model 145 of 180\n",
      "Training model 146 of 180\n",
      "Training model 147 of 180\n",
      "Training model 148 of 180\n",
      "Training model 149 of 180\n",
      "Training model 150 of 180\n",
      "Training model 151 of 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 152 of 180\n",
      "Training model 153 of 180\n",
      "Training model 154 of 180\n",
      "Training model 155 of 180\n",
      "Training model 156 of 180\n",
      "Training model 157 of 180\n",
      "Training model 158 of 180\n",
      "Training model 159 of 180\n",
      "Training model 160 of 180\n",
      "Training model 161 of 180\n",
      "Training model 162 of 180\n",
      "Training model 163 of 180\n",
      "Training model 164 of 180\n",
      "Training model 165 of 180\n",
      "Training model 166 of 180\n",
      "Training model 167 of 180\n",
      "Training model 168 of 180\n",
      "Training model 169 of 180\n",
      "Training model 170 of 180\n",
      "Training model 171 of 180\n",
      "Training model 172 of 180\n",
      "Training model 173 of 180\n",
      "Training model 174 of 180\n",
      "Training model 175 of 180\n",
      "Training model 176 of 180\n",
      "Training model 177 of 180\n",
      "Training model 178 of 180\n",
      "Training model 179 of 180\n",
      "Training model 180 of 180\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_rawdata = test_multiple_models(x_train = X_raw_train,\n",
    "                                                 y_train = y_raw_train,\n",
    "                                                 x_test = X_raw_test,\n",
    "                                                 y_test = y_raw_test,\n",
    "                                                 modelgrid = grid_simple_NN_rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6fd1af19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_split</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  learning_rate val_split epochs  accuracy  TPR  \\\n",
       "0    simple_NN_rawdata_model_1       0.010000      None      8  1.000000  1.0   \n",
       "1    simple_NN_rawdata_model_2       0.010000      None      4  1.000000  1.0   \n",
       "2    simple_NN_rawdata_model_1       0.010000      None      4  1.000000  1.0   \n",
       "3    simple_NN_rawdata_model_1       0.010000      None     10  1.000000  1.0   \n",
       "4    simple_NN_rawdata_model_2       0.010000      None      8  1.000000  1.0   \n",
       "..                         ...            ...       ...    ...       ...  ...   \n",
       "175  simple_NN_rawdata_model_2       0.000100      None      3  0.166667  0.3   \n",
       "176  simple_NN_rawdata_model_1       0.000001      None      4  0.111111  0.1   \n",
       "177  simple_NN_rawdata_model_1       0.000010      0.33      4  0.055556  0.1   \n",
       "178  simple_NN_rawdata_model_2       0.000010      None      4  0.055556  0.0   \n",
       "179  simple_NN_rawdata_model_1       0.000010      0.33      3  0.000000  0.0   \n",
       "\n",
       "       TNR  \n",
       "0    1.000  \n",
       "1    1.000  \n",
       "2    1.000  \n",
       "3    1.000  \n",
       "4    1.000  \n",
       "..     ...  \n",
       "175  0.000  \n",
       "176  0.125  \n",
       "177  0.000  \n",
       "178  0.125  \n",
       "179  0.000  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_simple_NN_rawdata = results_simple_NN_rawdata.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_rawdata.to_csv(folderpath_results + \"results_simple_NN_rawdata.csv\",\n",
    "                                              encoding = 'utf-8',\n",
    "                                              index = False)\n",
    "\n",
    "results_simple_NN_rawdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a59072",
   "metadata": {},
   "source": [
    "# Neural Networks on Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eecf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket01 scaledBetween\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledBetween_H1_train, X_b01_scaledBetween_H1_test, Y_b01_scaledBetween_H1_train, Y_b01_scaledBetween_H1_test = train_test_split(avgPL_bucket01_H1_scaledBetween,\n",
    "                                                                                                                                        Y,\n",
    "                                                                                                                                        test_size=0.2)\n",
    "\n",
    "# Bucket01 Unscaled\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_unscaled_H1_train, X_b01_unscaled_H1_test, Y_b01_unscaled_H1_train, Y_b01_unscaled_H1_test = train_test_split(avgPL_bucket01_H1_unscaled,\n",
    "                                                                                                                    Y,\n",
    "                                                                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c1493",
   "metadata": {},
   "source": [
    "### NN Models for Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de981ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_NN_PL_model_1():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=[2000,]))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_PL_model_2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=[2000,]))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_PL_model_3():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=[2000,]))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9e3c8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_NN_PL_models = [simple_NN_PL_model_1, simple_NN_PL_model_2, simple_NN_PL_model_3]\n",
    "simple_NN_PL_learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "simple_NN_PL_validation_splits = [0.33, None]\n",
    "simple_NN_PL_epochs = [3, 5, 8, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608e3f9",
   "metadata": {},
   "source": [
    "### Train Models on Persistence Landscapes (scaledBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "639e1af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>models_to_combine</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f9b99e0f160&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f9b99e0f1f0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f9b99e0f160&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f9b99e0f1f0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f9b99e0f160&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f9b99e0f1f0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model models_to_combine  \\\n",
       "0  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "1  <function simple_NN_PL_model_2 at 0x7f9b99e0f160>              None   \n",
       "2  <function simple_NN_PL_model_3 at 0x7f9b99e0f1f0>              None   \n",
       "3  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "4  <function simple_NN_PL_model_2 at 0x7f9b99e0f160>              None   \n",
       "5  <function simple_NN_PL_model_3 at 0x7f9b99e0f1f0>              None   \n",
       "6  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "7  <function simple_NN_PL_model_2 at 0x7f9b99e0f160>              None   \n",
       "8  <function simple_NN_PL_model_3 at 0x7f9b99e0f1f0>              None   \n",
       "9  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "\n",
       "  learning_rate validation_split epochs  \n",
       "0          0.01             0.33      3  \n",
       "1          0.01             0.33      3  \n",
       "2          0.01             0.33      3  \n",
       "3         0.001             0.33      3  \n",
       "4         0.001             0.33      3  \n",
       "5         0.001             0.33      3  \n",
       "6        0.0001             0.33      3  \n",
       "7        0.0001             0.33      3  \n",
       "8        0.0001             0.33      3  \n",
       "9       0.00001             0.33      3  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_simple_NN_pl_scaledBetween = create_modelgrid(models = simple_NN_PL_models,\n",
    "                                                   learning_rates = simple_NN_PL_learning_rates,\n",
    "                                                   validation_splits = simple_NN_PL_validation_splits,\n",
    "                                                   epochs = simple_NN_PL_epochs)\n",
    "grid_simple_NN_pl_scaledBetween.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b7aaef07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 180\n",
      "Training model 2 of 180\n",
      "Training model 3 of 180\n",
      "Training model 4 of 180\n",
      "Training model 5 of 180\n",
      "Training model 6 of 180\n",
      "Training model 7 of 180\n",
      "Training model 8 of 180\n",
      "Training model 9 of 180\n",
      "Training model 10 of 180\n",
      "Training model 11 of 180\n",
      "Training model 12 of 180\n",
      "Training model 13 of 180\n",
      "Training model 14 of 180\n",
      "Training model 15 of 180\n",
      "Training model 16 of 180\n",
      "Training model 17 of 180\n",
      "Training model 18 of 180\n",
      "Training model 19 of 180\n",
      "Training model 20 of 180\n",
      "Training model 21 of 180\n",
      "Training model 22 of 180\n",
      "Training model 23 of 180\n",
      "Training model 24 of 180\n",
      "Training model 25 of 180\n",
      "Training model 26 of 180\n",
      "Training model 27 of 180\n",
      "Training model 28 of 180\n",
      "Training model 29 of 180\n",
      "Training model 30 of 180\n",
      "Training model 31 of 180\n",
      "Training model 32 of 180\n",
      "Training model 33 of 180\n",
      "Training model 34 of 180\n",
      "Training model 35 of 180\n",
      "Training model 36 of 180\n",
      "Training model 37 of 180\n",
      "Training model 38 of 180\n",
      "Training model 39 of 180\n",
      "Training model 40 of 180\n",
      "Training model 41 of 180\n",
      "Training model 42 of 180\n",
      "Training model 43 of 180\n",
      "Training model 44 of 180\n",
      "Training model 45 of 180\n",
      "Training model 46 of 180\n",
      "Training model 47 of 180\n",
      "Training model 48 of 180\n",
      "Training model 49 of 180\n",
      "Training model 50 of 180\n",
      "Training model 51 of 180\n",
      "Training model 52 of 180\n",
      "Training model 53 of 180\n",
      "Training model 54 of 180\n",
      "Training model 55 of 180\n",
      "Training model 56 of 180\n",
      "Training model 57 of 180\n",
      "Training model 58 of 180\n",
      "Training model 59 of 180\n",
      "Training model 60 of 180\n",
      "Training model 61 of 180\n",
      "Training model 62 of 180\n",
      "Training model 63 of 180\n",
      "Training model 64 of 180\n",
      "Training model 65 of 180\n",
      "Training model 66 of 180\n",
      "Training model 67 of 180\n",
      "Training model 68 of 180\n",
      "Training model 69 of 180\n",
      "Training model 70 of 180\n",
      "Training model 71 of 180\n",
      "Training model 72 of 180\n",
      "Training model 73 of 180\n",
      "Training model 74 of 180\n",
      "Training model 75 of 180\n",
      "Training model 76 of 180\n",
      "Training model 77 of 180\n",
      "Training model 78 of 180\n",
      "Training model 79 of 180\n",
      "Training model 80 of 180\n",
      "Training model 81 of 180\n",
      "Training model 82 of 180\n",
      "Training model 83 of 180\n",
      "Training model 84 of 180\n",
      "Training model 85 of 180\n",
      "Training model 86 of 180\n",
      "Training model 87 of 180\n",
      "Training model 88 of 180\n",
      "Training model 89 of 180\n",
      "Training model 90 of 180\n",
      "Training model 91 of 180\n",
      "Training model 92 of 180\n",
      "Training model 93 of 180\n",
      "Training model 94 of 180\n",
      "Training model 95 of 180\n",
      "Training model 96 of 180\n",
      "Training model 97 of 180\n",
      "Training model 98 of 180\n",
      "Training model 99 of 180\n",
      "Training model 100 of 180\n",
      "Training model 101 of 180\n",
      "Training model 102 of 180\n",
      "Training model 103 of 180\n",
      "Training model 104 of 180\n",
      "Training model 105 of 180\n",
      "Training model 106 of 180\n",
      "Training model 107 of 180\n",
      "Training model 108 of 180\n",
      "Training model 109 of 180\n",
      "Training model 110 of 180\n",
      "Training model 111 of 180\n",
      "Training model 112 of 180\n",
      "Training model 113 of 180\n",
      "Training model 114 of 180\n",
      "Training model 115 of 180\n",
      "Training model 116 of 180\n",
      "Training model 117 of 180\n",
      "Training model 118 of 180\n",
      "Training model 119 of 180\n",
      "Training model 120 of 180\n",
      "Training model 121 of 180\n",
      "Training model 122 of 180\n",
      "Training model 123 of 180\n",
      "Training model 124 of 180\n",
      "Training model 125 of 180\n",
      "Training model 126 of 180\n",
      "Training model 127 of 180\n",
      "Training model 128 of 180\n",
      "Training model 129 of 180\n",
      "Training model 130 of 180\n",
      "Training model 131 of 180\n",
      "Training model 132 of 180\n",
      "Training model 133 of 180\n",
      "Training model 134 of 180\n",
      "Training model 135 of 180\n",
      "Training model 136 of 180\n",
      "Training model 137 of 180\n",
      "Training model 138 of 180\n",
      "Training model 139 of 180\n",
      "Training model 140 of 180\n",
      "Training model 141 of 180\n",
      "Training model 142 of 180\n",
      "Training model 143 of 180\n",
      "Training model 144 of 180\n",
      "Training model 145 of 180\n",
      "Training model 146 of 180\n",
      "Training model 147 of 180\n",
      "Training model 148 of 180\n",
      "Training model 149 of 180\n",
      "Training model 150 of 180\n",
      "Training model 151 of 180\n",
      "Training model 152 of 180\n",
      "Training model 153 of 180\n",
      "Training model 154 of 180\n",
      "Training model 155 of 180\n",
      "Training model 156 of 180\n",
      "Training model 157 of 180\n",
      "Training model 158 of 180\n",
      "Training model 159 of 180\n",
      "Training model 160 of 180\n",
      "Training model 161 of 180\n",
      "Training model 162 of 180\n",
      "Training model 163 of 180\n",
      "Training model 164 of 180\n",
      "Training model 165 of 180\n",
      "Training model 166 of 180\n",
      "Training model 167 of 180\n",
      "Training model 168 of 180\n",
      "Training model 169 of 180\n",
      "Training model 170 of 180\n",
      "Training model 171 of 180\n",
      "Training model 172 of 180\n",
      "Training model 173 of 180\n",
      "Training model 174 of 180\n",
      "Training model 175 of 180\n",
      "Training model 176 of 180\n",
      "Training model 177 of 180\n",
      "Training model 178 of 180\n",
      "Training model 179 of 180\n",
      "Training model 180 of 180\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_b01_scaledBetween_H1 = test_multiple_models(x_train = np.asarray(X_b01_scaledBetween_H1_train),\n",
    "                                                              y_train = np.asarray(Y_b01_scaledBetween_H1_train),\n",
    "                                                              x_test = np.asarray(X_b01_scaledBetween_H1_test),\n",
    "                                                              y_test = np.asarray(Y_b01_scaledBetween_H1_test),\n",
    "                                                              modelgrid = grid_simple_NN_pl_scaledBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "302c646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_split</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  learning_rate val_split epochs  accuracy       TPR  \\\n",
       "0    simple_NN_PL_model_3       0.000010      None     20  1.000000  1.000000   \n",
       "1    simple_NN_PL_model_3       0.010000      None     50  1.000000  1.000000   \n",
       "2    simple_NN_PL_model_1       0.010000      None     50  1.000000  1.000000   \n",
       "3    simple_NN_PL_model_2       0.010000      None     50  1.000000  1.000000   \n",
       "4    simple_NN_PL_model_3       0.001000      None     20  0.916667  0.833333   \n",
       "..                    ...            ...       ...    ...       ...       ...   \n",
       "175  simple_NN_PL_model_3       0.000010      None      3  0.333333  0.000000   \n",
       "176  simple_NN_PL_model_2       0.000001      None      8  0.333333  0.333333   \n",
       "177  simple_NN_PL_model_2       0.000010      0.33     50  0.250000  0.000000   \n",
       "178  simple_NN_PL_model_1       0.000001      0.33      5  0.250000  0.500000   \n",
       "179  simple_NN_PL_model_2       0.000001      0.33     10  0.250000  0.166667   \n",
       "\n",
       "          TNR  \n",
       "0    1.000000  \n",
       "1    1.000000  \n",
       "2    1.000000  \n",
       "3    1.000000  \n",
       "4    1.000000  \n",
       "..        ...  \n",
       "175  0.666667  \n",
       "176  0.333333  \n",
       "177  0.500000  \n",
       "178  0.000000  \n",
       "179  0.333333  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_simple_NN_b01_scaledBetween_H1 = results_simple_NN_b01_scaledBetween_H1.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_b01_scaledBetween_H1.to_csv(folderpath_results + \"results_simple_NN_b01_scaledBetween_H1.csv\",\n",
    "                                              encoding='utf-8',\n",
    "                                              index=False)\n",
    "\n",
    "# Print results\n",
    "results_simple_NN_b01_scaledBetween_H1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e124ac7",
   "metadata": {},
   "source": [
    "### Train Models on Persistence Landscapes for H1 (unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "138d038a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>models_to_combine</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f9b99e0f160&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f9b99e0f1f0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f9b99e0f0d0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f9b99e0f160&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model models_to_combine  \\\n",
       "0  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "1  <function simple_NN_PL_model_2 at 0x7f9b99e0f160>              None   \n",
       "2  <function simple_NN_PL_model_3 at 0x7f9b99e0f1f0>              None   \n",
       "3  <function simple_NN_PL_model_1 at 0x7f9b99e0f0d0>              None   \n",
       "4  <function simple_NN_PL_model_2 at 0x7f9b99e0f160>              None   \n",
       "\n",
       "  learning_rate validation_split epochs  \n",
       "0          0.01             0.33      3  \n",
       "1          0.01             0.33      3  \n",
       "2          0.01             0.33      3  \n",
       "3         0.001             0.33      3  \n",
       "4         0.001             0.33      3  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_simple_NN_pl_unscaled = create_modelgrid(models = simple_NN_PL_models,\n",
    "                                              learning_rates = simple_NN_PL_learning_rates,\n",
    "                                              validation_splits = simple_NN_PL_validation_splits,\n",
    "                                              epochs = simple_NN_PL_epochs)\n",
    "grid_simple_NN_pl_unscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17309dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 180\n",
      "Training model 2 of 180\n",
      "Training model 3 of 180\n",
      "Training model 4 of 180\n",
      "Training model 5 of 180\n",
      "Training model 6 of 180\n",
      "Training model 7 of 180\n",
      "Training model 8 of 180\n",
      "Training model 9 of 180\n",
      "Training model 10 of 180\n",
      "Training model 11 of 180\n",
      "Training model 12 of 180\n",
      "Training model 13 of 180\n",
      "Training model 14 of 180\n",
      "Training model 15 of 180\n",
      "Training model 16 of 180\n",
      "Training model 17 of 180\n",
      "Training model 18 of 180\n",
      "Training model 19 of 180\n",
      "Training model 20 of 180\n",
      "Training model 21 of 180\n",
      "Training model 22 of 180\n",
      "Training model 23 of 180\n",
      "Training model 24 of 180\n",
      "Training model 25 of 180\n",
      "Training model 26 of 180\n",
      "Training model 27 of 180\n",
      "Training model 28 of 180\n",
      "Training model 29 of 180\n",
      "Training model 30 of 180\n",
      "Training model 31 of 180\n",
      "Training model 32 of 180\n",
      "Training model 33 of 180\n",
      "Training model 34 of 180\n",
      "Training model 35 of 180\n",
      "Training model 36 of 180\n",
      "Training model 37 of 180\n",
      "Training model 38 of 180\n",
      "Training model 39 of 180\n",
      "Training model 40 of 180\n",
      "Training model 41 of 180\n",
      "Training model 42 of 180\n",
      "Training model 43 of 180\n",
      "Training model 44 of 180\n",
      "Training model 45 of 180\n",
      "Training model 46 of 180\n",
      "Training model 47 of 180\n",
      "Training model 48 of 180\n",
      "Training model 49 of 180\n",
      "Training model 50 of 180\n",
      "Training model 51 of 180\n",
      "Training model 52 of 180\n",
      "Training model 53 of 180\n",
      "Training model 54 of 180\n",
      "Training model 55 of 180\n",
      "Training model 56 of 180\n",
      "Training model 57 of 180\n",
      "Training model 58 of 180\n",
      "Training model 59 of 180\n",
      "Training model 60 of 180\n",
      "Training model 61 of 180\n",
      "Training model 62 of 180\n",
      "Training model 63 of 180\n",
      "Training model 64 of 180\n",
      "Training model 65 of 180\n",
      "Training model 66 of 180\n",
      "Training model 67 of 180\n",
      "Training model 68 of 180\n",
      "Training model 69 of 180\n",
      "Training model 70 of 180\n",
      "Training model 71 of 180\n",
      "Training model 72 of 180\n",
      "Training model 73 of 180\n",
      "Training model 74 of 180\n",
      "Training model 75 of 180\n",
      "Training model 76 of 180\n",
      "Training model 77 of 180\n",
      "Training model 78 of 180\n",
      "Training model 79 of 180\n",
      "Training model 80 of 180\n",
      "Training model 81 of 180\n",
      "Training model 82 of 180\n",
      "Training model 83 of 180\n",
      "Training model 84 of 180\n",
      "Training model 85 of 180\n",
      "Training model 86 of 180\n",
      "Training model 87 of 180\n",
      "Training model 88 of 180\n",
      "Training model 89 of 180\n",
      "Training model 90 of 180\n",
      "Training model 91 of 180\n",
      "Training model 92 of 180\n",
      "Training model 93 of 180\n",
      "Training model 94 of 180\n",
      "Training model 95 of 180\n",
      "Training model 96 of 180\n",
      "Training model 97 of 180\n",
      "Training model 98 of 180\n",
      "Training model 99 of 180\n",
      "Training model 100 of 180\n",
      "Training model 101 of 180\n",
      "Training model 102 of 180\n",
      "Training model 103 of 180\n",
      "Training model 104 of 180\n",
      "Training model 105 of 180\n",
      "Training model 106 of 180\n",
      "Training model 107 of 180\n",
      "Training model 108 of 180\n",
      "Training model 109 of 180\n",
      "Training model 110 of 180\n",
      "Training model 111 of 180\n",
      "Training model 112 of 180\n",
      "Training model 113 of 180\n",
      "Training model 114 of 180\n",
      "Training model 115 of 180\n",
      "Training model 116 of 180\n",
      "Training model 117 of 180\n",
      "Training model 118 of 180\n",
      "Training model 119 of 180\n",
      "Training model 120 of 180\n",
      "Training model 121 of 180\n",
      "Training model 122 of 180\n",
      "Training model 123 of 180\n",
      "Training model 124 of 180\n",
      "Training model 125 of 180\n",
      "Training model 126 of 180\n",
      "Training model 127 of 180\n",
      "Training model 128 of 180\n",
      "Training model 129 of 180\n",
      "Training model 130 of 180\n",
      "Training model 131 of 180\n",
      "Training model 132 of 180\n",
      "Training model 133 of 180\n",
      "Training model 134 of 180\n",
      "Training model 135 of 180\n",
      "Training model 136 of 180\n",
      "Training model 137 of 180\n",
      "Training model 138 of 180\n",
      "Training model 139 of 180\n",
      "Training model 140 of 180\n",
      "Training model 141 of 180\n",
      "Training model 142 of 180\n",
      "Training model 143 of 180\n",
      "Training model 144 of 180\n",
      "Training model 145 of 180\n",
      "Training model 146 of 180\n",
      "Training model 147 of 180\n",
      "Training model 148 of 180\n",
      "Training model 149 of 180\n",
      "Training model 150 of 180\n",
      "Training model 151 of 180\n",
      "Training model 152 of 180\n",
      "Training model 153 of 180\n",
      "Training model 154 of 180\n",
      "Training model 155 of 180\n",
      "Training model 156 of 180\n",
      "Training model 157 of 180\n",
      "Training model 158 of 180\n",
      "Training model 159 of 180\n",
      "Training model 160 of 180\n",
      "Training model 161 of 180\n",
      "Training model 162 of 180\n",
      "Training model 163 of 180\n",
      "Training model 164 of 180\n",
      "Training model 165 of 180\n",
      "Training model 166 of 180\n",
      "Training model 167 of 180\n",
      "Training model 168 of 180\n",
      "Training model 169 of 180\n",
      "Training model 170 of 180\n",
      "Training model 171 of 180\n",
      "Training model 172 of 180\n",
      "Training model 173 of 180\n",
      "Training model 174 of 180\n",
      "Training model 175 of 180\n",
      "Training model 176 of 180\n",
      "Training model 177 of 180\n",
      "Training model 178 of 180\n",
      "Training model 179 of 180\n",
      "Training model 180 of 180\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_b01_unscaled_H1 = test_multiple_models(x_train = np.asarray(X_b01_unscaled_H1_train),\n",
    "                                                             y_train = np.asarray(Y_b01_unscaled_H1_train),\n",
    "                                                             x_test = np.asarray(X_b01_unscaled_H1_test),\n",
    "                                                             y_test = np.asarray(Y_b01_unscaled_H1_test),\n",
    "                                                             modelgrid = grid_simple_NN_pl_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0541521d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_split</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  learning_rate val_split epochs  accuracy  TPR  \\\n",
       "0    simple_NN_PL_model_2       0.010000      0.33     50  0.916667  1.0   \n",
       "1    simple_NN_PL_model_1       0.010000      0.33     10  0.666667  0.0   \n",
       "2    simple_NN_PL_model_2       0.010000      None     20  0.666667  0.0   \n",
       "3    simple_NN_PL_model_2       0.000001      None     20  0.666667  0.0   \n",
       "4    simple_NN_PL_model_3       0.000001      0.33     50  0.666667  0.0   \n",
       "..                    ...            ...       ...    ...       ...  ...   \n",
       "175  simple_NN_PL_model_1       0.001000      0.33      3  0.333333  1.0   \n",
       "176  simple_NN_PL_model_3       0.000100      None     50  0.333333  1.0   \n",
       "177  simple_NN_PL_model_1       0.001000      None     50  0.333333  1.0   \n",
       "178  simple_NN_PL_model_2       0.001000      None     50  0.333333  1.0   \n",
       "179  simple_NN_PL_model_1       0.010000      None     10  0.333333  1.0   \n",
       "\n",
       "       TNR  \n",
       "0    0.875  \n",
       "1    1.000  \n",
       "2    1.000  \n",
       "3    1.000  \n",
       "4    1.000  \n",
       "..     ...  \n",
       "175  0.000  \n",
       "176  0.000  \n",
       "177  0.000  \n",
       "178  0.000  \n",
       "179  0.000  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_simple_NN_b01_unscaled_H1 = results_simple_NN_b01_unscaled_H1.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_b01_unscaled_H1.to_csv(folderpath_results + \"results_simple_NN_b01_unscaled_H1.csv\",\n",
    "                                              encoding='utf-8',\n",
    "                                              index=False)\n",
    "\n",
    "results_simple_NN_b01_unscaled_H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82367bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
