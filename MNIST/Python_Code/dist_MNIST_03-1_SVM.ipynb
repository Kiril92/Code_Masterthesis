{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import random\n",
    "import math\n",
    "import dill\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import seed\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import device\n",
    "\n",
    "# Tensorflow soll auf CPU und nicht auf der GPU laufen\n",
    "device(\"cpu:0\")\n",
    "\n",
    "# f√ºr GPU:\n",
    "# tf.device(\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../../_functions/functions.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3c67a",
   "metadata": {},
   "source": [
    "### Load MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MNIST von sklearn mit 28x28 Pixel\n",
    "# from sklearn.datasets import fetch_openml\n",
    "\n",
    "# # Load data from https://www.openml.org/d/554\n",
    "# X_raw, y_raw = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data that was saved on this system\n",
    "\n",
    "raw_data = load_file(file = \"../raw_data/MNIST_images.pkl\")\n",
    "X_raw = raw_data[0]\n",
    "y_raw = raw_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on indices of '0' and '8'\n",
    "indices = (y_raw == '0') | (y_raw == '8')\n",
    "\n",
    "# Reshape into squares\n",
    "X_raw = X_raw[indices, :]\n",
    "X = X_raw.reshape([sum(indices), 28, 28])\n",
    "\n",
    "y = y_raw[indices]\n",
    "\n",
    "# Factorize into dichotomous variable\n",
    "Y = pd.factorize(y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We analyse 1998 samples\n",
    "np.unique(y[0:1998], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d954d8",
   "metadata": {},
   "source": [
    "### Load Persistences Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_folderpath = \"../persistence_landscapes_averages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingelesenen Dateien\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledWithin*.pkl\")[-1])\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledWithin*.pkl\")[-1])\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledBetween*.pkl\")[-1])\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledBetween*.pkl\")[-1])\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H0_unscaled*.pkl\")[-1])\n",
    "print(glob.glob(pl_folderpath + \"avgPL_bucket01_H1_unscaled*.pkl\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f815523",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPL_bucket01_H0_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledWithin*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledWithin*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledBetween*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledBetween*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_unscaled = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_unscaled*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_unscaled = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_unscaled*.pkl\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for persistence landscapes\n",
    "pl_resolution = 250\n",
    "pl_num_landscapes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989417d2",
   "metadata": {},
   "source": [
    "### Train-Test-Splits Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e20091",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_samples = len(avgPL_bucket01_H0_scaledBetween)\n",
    "relevant_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa49d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket01 scaledBetween\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledBetween_H0_train, X_b01_scaledBetween_H0_test, Y_b01_scaledBetween_H0_train, Y_b01_scaledBetween_H0_test = train_test_split(avgPL_bucket01_H0_scaledBetween,\n",
    "                                                                                                                                        Y[0:relevant_samples],\n",
    "                                                                                                                                        test_size=0.2)\n",
    "\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledBetween_H1_train, X_b01_scaledBetween_H1_test, Y_b01_scaledBetween_H1_train, Y_b01_scaledBetween_H1_test = train_test_split(avgPL_bucket01_H1_scaledBetween,\n",
    "                                                                                                                                        Y[0:relevant_samples],\n",
    "                                                                                                                                        test_size=0.2)\n",
    "\n",
    "# Bucket01 scaledWithin\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledWithin_H0_train, X_b01_scaledWithin_H0_test, Y_b01_scaledWithin_H0_train, Y_b01_scaledWithin_H0_test = train_test_split(avgPL_bucket01_H0_scaledWithin,\n",
    "                                                                                                                                    Y[0:relevant_samples],\n",
    "                                                                                                                                    test_size=0.2)\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledWithin_H1_train, X_b01_scaledWithin_H1_test, Y_b01_scaledWithin_H1_train, Y_b01_scaledWithin_H1_test = train_test_split(avgPL_bucket01_H1_scaledWithin,\n",
    "                                                                                                                                    Y[0:relevant_samples],\n",
    "                                                                                                                                    test_size=0.2)\n",
    "\n",
    "# Bucket01 Unscaled\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_unscaled_H0_train, X_b01_unscaled_H0_test, Y_b01_unscaled_H0_train, Y_b01_unscaled_H0_test = train_test_split(avgPL_bucket01_H0_unscaled,\n",
    "                                                                                                                    Y[0:relevant_samples],\n",
    "                                                                                                                    test_size=0.2)\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_unscaled_H1_train, X_b01_unscaled_H1_test, Y_b01_unscaled_H1_train, Y_b01_unscaled_H1_test = train_test_split(avgPL_bucket01_H1_unscaled,\n",
    "                                                                                                                    Y[0:relevant_samples],\n",
    "                                                                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f34a2",
   "metadata": {},
   "source": [
    "# Machine Learning Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c7971",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath_results = \"../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with data in vector format\n",
    "image_vector_size = 28*28\n",
    "\n",
    "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X[0:len(avgPL_bucket01_H1_scaledBetween)].reshape(len(avgPL_bucket01_H1_scaledBetween), image_vector_size),\n",
    "                                                                    np.asarray(y[0:len(avgPL_bucket01_H1_scaledBetween)]),\n",
    "                                                                    test_size = 0.3,\n",
    "                                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e7823",
   "metadata": {},
   "source": [
    "##### SVM (raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd44915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - Rawdata\n",
    "clf = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='linear')).fit(X_raw_train, y_raw_train.ravel())\n",
    "\n",
    "print(\"Accuracy SVM on raw data: \" + str(clf.score(X_raw_test, y_raw_test))[:10])\n",
    "\n",
    "evaluation(y_test = y_raw_test,\n",
    "           y_pred = clf.predict(X_raw_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_rawdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - Rawdata\n",
    "clf = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='rbf')).fit(X_raw_train, y_raw_train.ravel())\n",
    "\n",
    "print(\"Accuracy SVM on raw data: \" + str(clf.score(X_raw_test, y_raw_test))[:10])\n",
    "\n",
    "evaluation(y_test = y_raw_test,\n",
    "           y_pred = clf.predict(X_raw_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_rawdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf41392",
   "metadata": {},
   "source": [
    "##### SVM (Persistent Landscapes - scaledWithin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b260123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H0 - scaledWithin\n",
    "clf_pl_h0 = make_pipeline(StandardScaler(), \n",
    "                       SVC(kernel='linear')).fit(X_b01_scaledWithin_H0_train, Y_b01_scaledWithin_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM on peristence landscapes (scaled within sample): \" + str(clf_pl_h0.score(X_b01_scaledWithin_H0_test, Y_b01_scaledWithin_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledWithin_H0_test,\n",
    "           y_pred = clf_pl_h0.predict(X_b01_scaledWithin_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H0_scaledWithin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H1 - scaledWithin\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                       SVC(kernel='linear')).fit(X_b01_scaledWithin_H1_train, Y_b01_scaledWithin_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM on peristence landscapes (scaled within sample): \" + str(clf_pl.score(X_b01_scaledWithin_H1_test, Y_b01_scaledWithin_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledWithin_H1_test,\n",
    "           y_pred = clf_pl.predict(X_b01_scaledWithin_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H1_scaledWithin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d94b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H0 - scaledWithin\n",
    "clf_pl_h0 = make_pipeline(StandardScaler(), \n",
    "                       SVC(kernel='rbf')).fit(X_b01_scaledWithin_H0_train, Y_b01_scaledWithin_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM on peristence landscapes (scaled within sample): \" + str(clf_pl_h0.score(X_b01_scaledWithin_H0_test, Y_b01_scaledWithin_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledWithin_H0_test,\n",
    "           y_pred = clf_pl_h0.predict(X_b01_scaledWithin_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H0_scaledWithin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639574ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H1 - scaledWithin\n",
    "clf_pl_rbf = make_pipeline(StandardScaler(), \n",
    "                       SVC(kernel='rbf')).fit(X_b01_scaledWithin_H1_train, Y_b01_scaledWithin_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM on peristence landscapes (scaled within sample): \" + str(clf_pl_rbf.score(X_b01_scaledWithin_H1_test, Y_b01_scaledWithin_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledWithin_H1_test,\n",
    "           y_pred = clf_pl_rbf.predict(X_b01_scaledWithin_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H1_scaledWithin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bb3f8",
   "metadata": {},
   "source": [
    "##### SVM (Persistent Landscapes - scaledBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f786ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H0 - scaledBetween\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='linear')).fit(X_b01_scaledBetween_H0_train, Y_b01_scaledBetween_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_scaledBetween_H0_test, Y_b01_scaledBetween_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledBetween_H0_test,\n",
    "           y_pred = clf_pl.predict(X_b01_scaledBetween_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H0_scaledBetween.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H1 - scaledBetween\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='linear')).fit(X_b01_scaledBetween_H1_train, Y_b01_scaledBetween_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_scaledBetween_H1_test, Y_b01_scaledBetween_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledBetween_H1_test,\n",
    "           y_pred = clf_pl.predict(X_b01_scaledBetween_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H1_scaledBetween.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H0 - scaledBetween\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='rbf')).fit(X_b01_scaledBetween_H0_train, Y_b01_scaledBetween_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_scaledBetween_H0_test, Y_b01_scaledBetween_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledBetween_H0_test,\n",
    "           y_pred = clf_pl.predict(X_b01_scaledBetween_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H0_scaledBetween.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H1 - scaledBetween\n",
    "clf_pl_rbf = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='rbf')).fit(X_b01_scaledBetween_H1_train, Y_b01_scaledBetween_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl_rbf.score(X_b01_scaledBetween_H1_test, Y_b01_scaledBetween_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_scaledBetween_H1_test,\n",
    "           y_pred = clf_pl_rbf.predict(X_b01_scaledBetween_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H1_scaledBetween.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70f402",
   "metadata": {},
   "source": [
    "##### SVM (Persistent Landscapes - unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H0 - unscaled\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='linear')).fit(X_b01_unscaled_H0_train, Y_b01_unscaled_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_unscaled_H0_test, Y_b01_unscaled_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_unscaled_H0_test,\n",
    "           y_pred = clf_pl.predict(X_b01_unscaled_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H0_unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec616cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - H1 - unscaled\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='linear')).fit(X_b01_unscaled_H1_train, Y_b01_unscaled_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_unscaled_H1_test, Y_b01_unscaled_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_unscaled_H1_test,\n",
    "           y_pred = clf_pl.predict(X_b01_unscaled_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_linear_H1_unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad264de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H0 - unscaled\n",
    "clf_pl = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='rbf')).fit(X_b01_unscaled_H0_train, Y_b01_unscaled_H0_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl.score(X_b01_unscaled_H0_test, Y_b01_unscaled_H0_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_unscaled_H0_test,\n",
    "           y_pred = clf_pl.predict(X_b01_unscaled_H0_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H0_unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF - H1 - unscaled\n",
    "clf_pl_rbf = make_pipeline(StandardScaler(), \n",
    "                    SVC(kernel='rbf')).fit(X_b01_unscaled_H1_train, Y_b01_unscaled_H1_train)\n",
    "\n",
    "print(\"Accuracy SVM : \" + str(clf_pl_rbf.score(X_b01_unscaled_H1_test, Y_b01_unscaled_H1_test))[:10])\n",
    "\n",
    "evaluation(y_test = Y_b01_unscaled_H1_test,\n",
    "           y_pred = clf_pl_rbf.predict(X_b01_unscaled_H1_test),\n",
    "           filename_csv = folderpath_results + \"svm_rbf_H1_unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fbc18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
