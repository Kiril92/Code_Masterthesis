{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28f5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 13:50:43.581668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-16 13:50:43.581701: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-16 13:50:44.520934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-16 13:50:44.520966: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-16 13:50:44.520985: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8141ff0cdda3): /proc/driver/nvidia/version does not exist\n",
      "2022-09-16 13:50:44.521182: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import shape\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import random\n",
    "import math\n",
    "import dill\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import seed\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import device\n",
    "\n",
    "# Tensorflow soll auf CPU und nicht auf der GPU laufen\n",
    "device(\"cpu:0\")\n",
    "\n",
    "# für GPU:\n",
    "# tf.device(\"gpu:0\")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cfebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../../_functions/functions.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3c67a",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bf0505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 8]), array([178, 174]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data that was saved on this system\n",
    "\n",
    "raw_data = load_file(file = \"../data/raw_data/raw_images_and_labels.pkl\")\n",
    "X = raw_data[0]\n",
    "y = raw_data[1]\n",
    "\n",
    "# Factorize into dichotomous variable\n",
    "Y = pd.factorize(y)[0]\n",
    "\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d954d8",
   "metadata": {},
   "source": [
    "### Load Persistences Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_folderpath = \"../data/persistence_landscapes_averages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f815523",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPL_bucket01_H0_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledWithin*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledWithin = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledWithin*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_scaledBetween*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_scaledBetween = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_scaledBetween*.pkl\")[-1])\n",
    "\n",
    "avgPL_bucket01_H0_unscaled = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H0_unscaled*.pkl\")[-1])\n",
    "avgPL_bucket01_H1_unscaled = load_file(file = glob.glob(pl_folderpath + \"avgPL_bucket01_H1_unscaled*.pkl\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f34a2",
   "metadata": {},
   "source": [
    "# Neural Networks on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2759ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_relevant_samples = len(avgPL_bucket01_H1_unscaled)\n",
    "no_relevant_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb651131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folderpath to store the results\n",
    "folderpath_results = \"../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a06ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for persistence landscapes\n",
    "pl_resolution = 250\n",
    "pl_num_landscapes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989417d2",
   "metadata": {},
   "source": [
    "### Train-Test-Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f516e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with data in vector format\n",
    "image_vector_size = 8*8\n",
    "\n",
    "# Transform into categorical variable\n",
    "Y_categorical = to_categorical(Y)\n",
    "\n",
    "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X.reshape(no_relevant_samples, image_vector_size),\n",
    "                                                                    np.asarray(Y),\n",
    "                                                                    test_size = 0.3,\n",
    "                                                                    random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab95895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_NN_rawdata_model_1():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=[8*8,]))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_rawdata_model_2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=[8*8,]))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_rawdata_model_3():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, activation='relu', input_shape=[8*8,]))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe40dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for gridsearch\n",
    "simple_NN_rawdata_model = [simple_NN_rawdata_model_1, simple_NN_rawdata_model_2, simple_NN_rawdata_model_3]\n",
    "simple_NN_rawdata_learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "simple_NN_rawdata_validation_splits = [0.33, None]\n",
    "simple_NN_rawdata_epochs = [3, 4, 5, 8, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f88ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_simple_NN_rawdata = create_modelgrid(models = simple_NN_rawdata_model,\n",
    "                                          learning_rates = simple_NN_rawdata_learning_rates,\n",
    "                                          validation_splits = simple_NN_rawdata_validation_splits,\n",
    "                                          epochs = simple_NN_rawdata_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d127fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 13:52:52.398618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2 of 210\n",
      "Training model 3 of 210\n",
      "Training model 4 of 210\n",
      "Training model 5 of 210\n",
      "Training model 6 of 210\n",
      "Training model 7 of 210\n",
      "Training model 8 of 210\n",
      "Training model 9 of 210\n",
      "Training model 10 of 210\n",
      "Training model 11 of 210\n",
      "Training model 12 of 210\n",
      "Training model 13 of 210\n",
      "Training model 14 of 210\n",
      "Training model 15 of 210\n",
      "Training model 16 of 210\n",
      "Training model 17 of 210\n",
      "Training model 18 of 210\n",
      "Training model 19 of 210\n",
      "Training model 20 of 210\n",
      "Training model 21 of 210\n",
      "Training model 22 of 210\n",
      "Training model 23 of 210\n",
      "Training model 24 of 210\n",
      "Training model 25 of 210\n",
      "Training model 26 of 210\n",
      "Training model 27 of 210\n",
      "Training model 28 of 210\n",
      "Training model 29 of 210\n",
      "Training model 30 of 210\n",
      "Training model 31 of 210\n",
      "Training model 32 of 210\n",
      "Training model 33 of 210\n",
      "Training model 34 of 210\n",
      "Training model 35 of 210\n",
      "Training model 36 of 210\n",
      "Training model 37 of 210\n",
      "Training model 38 of 210\n",
      "Training model 39 of 210\n",
      "Training model 40 of 210\n",
      "Training model 41 of 210\n",
      "Training model 42 of 210\n",
      "Training model 43 of 210\n",
      "Training model 44 of 210\n",
      "Training model 45 of 210\n",
      "Training model 46 of 210\n",
      "Training model 47 of 210\n",
      "Training model 48 of 210\n",
      "Training model 49 of 210\n",
      "Training model 50 of 210\n",
      "Training model 51 of 210\n",
      "Training model 52 of 210\n",
      "Training model 53 of 210\n",
      "Training model 54 of 210\n",
      "Training model 55 of 210\n",
      "Training model 56 of 210\n",
      "Training model 57 of 210\n",
      "Training model 58 of 210\n",
      "Training model 59 of 210\n",
      "Training model 60 of 210\n",
      "Training model 61 of 210\n",
      "Training model 62 of 210\n",
      "Training model 63 of 210\n",
      "Training model 64 of 210\n",
      "Training model 65 of 210\n",
      "Training model 66 of 210\n",
      "Training model 67 of 210\n",
      "Training model 68 of 210\n",
      "Training model 69 of 210\n",
      "Training model 70 of 210\n",
      "Training model 71 of 210\n",
      "Training model 72 of 210\n",
      "Training model 73 of 210\n",
      "Training model 74 of 210\n",
      "Training model 75 of 210\n",
      "Training model 76 of 210\n",
      "Training model 77 of 210\n",
      "Training model 78 of 210\n",
      "Training model 79 of 210\n",
      "Training model 80 of 210\n",
      "Training model 81 of 210\n",
      "Training model 82 of 210\n",
      "Training model 83 of 210\n",
      "Training model 84 of 210\n",
      "Training model 85 of 210\n",
      "Training model 86 of 210\n",
      "Training model 87 of 210\n",
      "Training model 88 of 210\n",
      "Training model 89 of 210\n",
      "Training model 90 of 210\n",
      "Training model 91 of 210\n",
      "Training model 92 of 210\n",
      "Training model 93 of 210\n",
      "Training model 94 of 210\n",
      "Training model 95 of 210\n",
      "Training model 96 of 210\n",
      "Training model 97 of 210\n",
      "Training model 98 of 210\n",
      "Training model 99 of 210\n",
      "Training model 100 of 210\n",
      "Training model 101 of 210\n",
      "Training model 102 of 210\n",
      "Training model 103 of 210\n",
      "Training model 104 of 210\n",
      "Training model 105 of 210\n",
      "Training model 106 of 210\n",
      "Training model 107 of 210\n",
      "Training model 108 of 210\n",
      "Training model 109 of 210\n",
      "Training model 110 of 210\n",
      "Training model 111 of 210\n",
      "Training model 112 of 210\n",
      "Training model 113 of 210\n",
      "Training model 114 of 210\n",
      "Training model 115 of 210\n",
      "Training model 116 of 210\n",
      "Training model 117 of 210\n",
      "Training model 118 of 210\n",
      "Training model 119 of 210\n",
      "Training model 120 of 210\n",
      "Training model 121 of 210\n",
      "Training model 122 of 210\n",
      "Training model 123 of 210\n",
      "Training model 124 of 210\n",
      "Training model 125 of 210\n",
      "Training model 126 of 210\n",
      "Training model 127 of 210\n",
      "Training model 128 of 210\n",
      "Training model 129 of 210\n",
      "Training model 130 of 210\n",
      "Training model 131 of 210\n",
      "Training model 132 of 210\n",
      "Training model 133 of 210\n",
      "Training model 134 of 210\n",
      "Training model 135 of 210\n",
      "Training model 136 of 210\n",
      "Training model 137 of 210\n",
      "Training model 138 of 210\n",
      "Training model 139 of 210\n",
      "Training model 140 of 210\n",
      "Training model 141 of 210\n",
      "Training model 142 of 210\n",
      "Training model 143 of 210\n",
      "Training model 144 of 210\n",
      "Training model 145 of 210\n",
      "Training model 146 of 210\n",
      "Training model 147 of 210\n",
      "Training model 148 of 210\n",
      "Training model 149 of 210\n",
      "Training model 150 of 210\n",
      "Training model 151 of 210\n",
      "Training model 152 of 210\n",
      "Training model 153 of 210\n",
      "Training model 154 of 210\n",
      "Training model 155 of 210\n",
      "Training model 156 of 210\n",
      "Training model 157 of 210\n",
      "Training model 158 of 210\n",
      "Training model 159 of 210\n",
      "Training model 160 of 210\n",
      "Training model 161 of 210\n",
      "Training model 162 of 210\n",
      "Training model 163 of 210\n",
      "Training model 164 of 210\n",
      "Training model 165 of 210\n",
      "Training model 166 of 210\n",
      "Training model 167 of 210\n",
      "Training model 168 of 210\n",
      "Training model 169 of 210\n",
      "Training model 170 of 210\n",
      "Training model 171 of 210\n",
      "Training model 172 of 210\n",
      "Training model 173 of 210\n",
      "Training model 174 of 210\n",
      "Training model 175 of 210\n",
      "Training model 176 of 210\n",
      "Training model 177 of 210\n",
      "Training model 178 of 210\n",
      "Training model 179 of 210\n",
      "Training model 180 of 210\n",
      "Training model 181 of 210\n",
      "Training model 182 of 210\n",
      "Training model 183 of 210\n",
      "Training model 184 of 210\n",
      "Training model 185 of 210\n",
      "Training model 186 of 210\n",
      "Training model 187 of 210\n",
      "Training model 188 of 210\n",
      "Training model 189 of 210\n",
      "Training model 190 of 210\n",
      "Training model 191 of 210\n",
      "Training model 192 of 210\n",
      "Training model 193 of 210\n",
      "Training model 194 of 210\n",
      "Training model 195 of 210\n",
      "Training model 196 of 210\n",
      "Training model 197 of 210\n",
      "Training model 198 of 210\n",
      "Training model 199 of 210\n",
      "Training model 200 of 210\n",
      "Training model 201 of 210\n",
      "Training model 202 of 210\n",
      "Training model 203 of 210\n",
      "Training model 204 of 210\n",
      "Training model 205 of 210\n",
      "Training model 206 of 210\n",
      "Training model 207 of 210\n",
      "Training model 208 of 210\n",
      "Training model 209 of 210\n",
      "Training model 210 of 210\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_rawdata = test_multiple_models(x_train = X_raw_train,\n",
    "                                                 y_train = y_raw_train,\n",
    "                                                 x_test = X_raw_test,\n",
    "                                                 y_test = y_raw_test,\n",
    "                                                 modelgrid = grid_simple_NN_rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb68eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_split</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_NN_rawdata_model_3</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>simple_NN_rawdata_model_2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>simple_NN_rawdata_model_1</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.204082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>simple_NN_rawdata_model_3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>simple_NN_rawdata_model_3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>8</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  learning_rate val_split epochs  accuracy  \\\n",
       "0    simple_NN_rawdata_model_1        0.01000      0.33      3  1.000000   \n",
       "1    simple_NN_rawdata_model_3        0.01000      None     10  1.000000   \n",
       "2    simple_NN_rawdata_model_1        0.01000      None     10  1.000000   \n",
       "3    simple_NN_rawdata_model_2        0.00100      0.33     10  1.000000   \n",
       "4    simple_NN_rawdata_model_1        0.00100      0.33     10  1.000000   \n",
       "..                         ...            ...       ...    ...       ...   \n",
       "205  simple_NN_rawdata_model_2        0.00001      0.33      4  0.216981   \n",
       "206  simple_NN_rawdata_model_1        0.00001      None     50  0.188679   \n",
       "207  simple_NN_rawdata_model_1        0.00010      0.33      5  0.132075   \n",
       "208  simple_NN_rawdata_model_3        0.00001      0.33     10  0.122642   \n",
       "209  simple_NN_rawdata_model_3        0.00001      0.33      8  0.047170   \n",
       "\n",
       "          TPR       TNR  \n",
       "0    1.000000  1.000000  \n",
       "1    1.000000  1.000000  \n",
       "2    1.000000  1.000000  \n",
       "3    1.000000  1.000000  \n",
       "4    1.000000  1.000000  \n",
       "..        ...       ...  \n",
       "205  0.122807  0.326531  \n",
       "206  0.157895  0.224490  \n",
       "207  0.070175  0.204082  \n",
       "208  0.000000  0.265306  \n",
       "209  0.070175  0.020408  \n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_simple_NN_rawdata = results_simple_NN_rawdata.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_rawdata.to_csv(folderpath_results + \"results_simple_NN_rawdata.csv\",\n",
    "                                              encoding = 'utf-8',\n",
    "                                              index = False)\n",
    "\n",
    "results_simple_NN_rawdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b94278",
   "metadata": {},
   "source": [
    "# Neural Networks on Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa49d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1 scaledBetween\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledBetween_H1_train, X_b01_scaledBetween_H1_test, Y_b01_scaledBetween_H1_train, Y_b01_scaledBetween_H1_test = train_test_split(avgPL_bucket01_H1_scaledBetween,\n",
    "                                                                                                                                        Y,\n",
    "                                                                                                                                        test_size=0.2)\n",
    "\n",
    "# H1 scaledWithin\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_scaledWithin_H1_train, X_b01_scaledWithin_H1_test, Y_b01_scaledWithin_H1_train, Y_b01_scaledWithin_H1_test = train_test_split(avgPL_bucket01_H1_scaledWithin,\n",
    "                                                                                                                                    Y,\n",
    "                                                                                                                                    test_size=0.2)\n",
    "\n",
    "# H1 Unscaled\n",
    "seed(999)\n",
    "# Train-Test-Split \n",
    "X_b01_unscaled_H1_train, X_b01_unscaled_H1_test, Y_b01_unscaled_H1_train, Y_b01_unscaled_H1_test = train_test_split(avgPL_bucket01_H1_unscaled,\n",
    "                                                                                                                    Y,\n",
    "                                                                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aa8b7",
   "metadata": {},
   "source": [
    "### NN Models for Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a326d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_NN_PL_model_1():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=[2500,]))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_PL_model_2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=[2500,]))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def simple_NN_PL_model_3():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=[2500,]))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "966b67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_NN_PL_models = [simple_NN_PL_model_1, simple_NN_PL_model_2, simple_NN_PL_model_3]\n",
    "simple_NN_PL_learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "simple_NN_PL_validation_splits = [0.33, None]\n",
    "simple_NN_PL_epochs = [3, 5, 8, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a5a3f",
   "metadata": {},
   "source": [
    "### Train Models on Persistence Landscapes (scaledBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53b00951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>models_to_combine</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f019c961e50&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f019ecfd5e0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f019c961e50&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f019ecfd5e0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f019c961e50&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f019ecfd5e0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model models_to_combine  \\\n",
       "0  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "1  <function simple_NN_PL_model_2 at 0x7f019c961e50>              None   \n",
       "2  <function simple_NN_PL_model_3 at 0x7f019ecfd5e0>              None   \n",
       "3  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "4  <function simple_NN_PL_model_2 at 0x7f019c961e50>              None   \n",
       "5  <function simple_NN_PL_model_3 at 0x7f019ecfd5e0>              None   \n",
       "6  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "7  <function simple_NN_PL_model_2 at 0x7f019c961e50>              None   \n",
       "8  <function simple_NN_PL_model_3 at 0x7f019ecfd5e0>              None   \n",
       "9  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "\n",
       "  learning_rate validation_split epochs  \n",
       "0          0.01             0.33      3  \n",
       "1          0.01             0.33      3  \n",
       "2          0.01             0.33      3  \n",
       "3         0.001             0.33      3  \n",
       "4         0.001             0.33      3  \n",
       "5         0.001             0.33      3  \n",
       "6        0.0001             0.33      3  \n",
       "7        0.0001             0.33      3  \n",
       "8        0.0001             0.33      3  \n",
       "9       0.00001             0.33      3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_simple_NN_pl_scaledBetween = create_modelgrid(models = simple_NN_PL_models,\n",
    "                                                   learning_rates = simple_NN_PL_learning_rates,\n",
    "                                                   validation_splits = simple_NN_PL_validation_splits,\n",
    "                                                   epochs = simple_NN_PL_epochs)\n",
    "grid_simple_NN_pl_scaledBetween.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd33c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 180\n",
      "Training model 2 of 180\n",
      "Training model 3 of 180\n",
      "Training model 4 of 180\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f019cb6eca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training model 5 of 180\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f019ca77940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training model 6 of 180\n",
      "Training model 7 of 180\n",
      "Training model 8 of 180\n",
      "Training model 9 of 180\n",
      "Training model 10 of 180\n",
      "Training model 11 of 180\n",
      "Training model 12 of 180\n",
      "Training model 13 of 180\n",
      "Training model 14 of 180\n",
      "Training model 15 of 180\n",
      "Training model 16 of 180\n",
      "Training model 17 of 180\n",
      "Training model 18 of 180\n",
      "Training model 19 of 180\n",
      "Training model 20 of 180\n",
      "Training model 21 of 180\n",
      "Training model 22 of 180\n",
      "Training model 23 of 180\n",
      "Training model 24 of 180\n",
      "Training model 25 of 180\n",
      "Training model 26 of 180\n",
      "Training model 27 of 180\n",
      "Training model 28 of 180\n",
      "Training model 29 of 180\n",
      "Training model 30 of 180\n",
      "Training model 31 of 180\n",
      "Training model 32 of 180\n",
      "Training model 33 of 180\n",
      "Training model 34 of 180\n",
      "Training model 35 of 180\n",
      "Training model 36 of 180\n",
      "Training model 37 of 180\n",
      "Training model 38 of 180\n",
      "Training model 39 of 180\n",
      "Training model 40 of 180\n",
      "Training model 41 of 180\n",
      "Training model 42 of 180\n",
      "Training model 43 of 180\n",
      "Training model 44 of 180\n",
      "Training model 45 of 180\n",
      "Training model 46 of 180\n",
      "Training model 47 of 180\n",
      "Training model 48 of 180\n",
      "Training model 49 of 180\n",
      "Training model 50 of 180\n",
      "Training model 51 of 180\n",
      "Training model 52 of 180\n",
      "Training model 53 of 180\n",
      "Training model 54 of 180\n",
      "Training model 55 of 180\n",
      "Training model 56 of 180\n",
      "Training model 57 of 180\n",
      "Training model 58 of 180\n",
      "Training model 59 of 180\n",
      "Training model 60 of 180\n",
      "Training model 61 of 180\n",
      "Training model 62 of 180\n",
      "Training model 63 of 180\n",
      "Training model 64 of 180\n",
      "Training model 65 of 180\n",
      "Training model 66 of 180\n",
      "Training model 67 of 180\n",
      "Training model 68 of 180\n",
      "Training model 69 of 180\n",
      "Training model 70 of 180\n",
      "Training model 71 of 180\n",
      "Training model 72 of 180\n",
      "Training model 73 of 180\n",
      "Training model 74 of 180\n",
      "Training model 75 of 180\n",
      "Training model 76 of 180\n",
      "Training model 77 of 180\n",
      "Training model 78 of 180\n",
      "Training model 79 of 180\n",
      "Training model 80 of 180\n",
      "Training model 81 of 180\n",
      "Training model 82 of 180\n",
      "Training model 83 of 180\n",
      "Training model 84 of 180\n",
      "Training model 85 of 180\n",
      "Training model 86 of 180\n",
      "Training model 87 of 180\n",
      "Training model 88 of 180\n",
      "Training model 89 of 180\n",
      "Training model 90 of 180\n",
      "Training model 91 of 180\n",
      "Training model 92 of 180\n",
      "Training model 93 of 180\n",
      "Training model 94 of 180\n",
      "Training model 95 of 180\n",
      "Training model 96 of 180\n",
      "Training model 97 of 180\n",
      "Training model 98 of 180\n",
      "Training model 99 of 180\n",
      "Training model 100 of 180\n",
      "Training model 101 of 180\n",
      "Training model 102 of 180\n",
      "Training model 103 of 180\n",
      "Training model 104 of 180\n",
      "Training model 105 of 180\n",
      "Training model 106 of 180\n",
      "Training model 107 of 180\n",
      "Training model 108 of 180\n",
      "Training model 109 of 180\n",
      "Training model 110 of 180\n",
      "Training model 111 of 180\n",
      "Training model 112 of 180\n",
      "Training model 113 of 180\n",
      "Training model 114 of 180\n",
      "Training model 115 of 180\n",
      "Training model 116 of 180\n",
      "Training model 117 of 180\n",
      "Training model 118 of 180\n",
      "Training model 119 of 180\n",
      "Training model 120 of 180\n",
      "Training model 121 of 180\n",
      "Training model 122 of 180\n",
      "Training model 123 of 180\n",
      "Training model 124 of 180\n",
      "Training model 125 of 180\n",
      "Training model 126 of 180\n",
      "Training model 127 of 180\n",
      "Training model 128 of 180\n",
      "Training model 129 of 180\n",
      "Training model 130 of 180\n",
      "Training model 131 of 180\n",
      "Training model 132 of 180\n",
      "Training model 133 of 180\n",
      "Training model 134 of 180\n",
      "Training model 135 of 180\n",
      "Training model 136 of 180\n",
      "Training model 137 of 180\n",
      "Training model 138 of 180\n",
      "Training model 139 of 180\n",
      "Training model 140 of 180\n",
      "Training model 141 of 180\n",
      "Training model 142 of 180\n",
      "Training model 143 of 180\n",
      "Training model 144 of 180\n",
      "Training model 145 of 180\n",
      "Training model 146 of 180\n",
      "Training model 147 of 180\n",
      "Training model 148 of 180\n",
      "Training model 149 of 180\n",
      "Training model 150 of 180\n",
      "Training model 151 of 180\n",
      "Training model 152 of 180\n",
      "Training model 153 of 180\n",
      "Training model 154 of 180\n",
      "Training model 155 of 180\n",
      "Training model 156 of 180\n",
      "Training model 157 of 180\n",
      "Training model 158 of 180\n",
      "Training model 159 of 180\n",
      "Training model 160 of 180\n",
      "Training model 161 of 180\n",
      "Training model 162 of 180\n",
      "Training model 163 of 180\n",
      "Training model 164 of 180\n",
      "Training model 165 of 180\n",
      "Training model 166 of 180\n",
      "Training model 167 of 180\n",
      "Training model 168 of 180\n",
      "Training model 169 of 180\n",
      "Training model 170 of 180\n",
      "Training model 171 of 180\n",
      "Training model 172 of 180\n",
      "Training model 173 of 180\n",
      "Training model 174 of 180\n",
      "Training model 175 of 180\n",
      "Training model 176 of 180\n",
      "Training model 177 of 180\n",
      "Training model 178 of 180\n",
      "Training model 179 of 180\n",
      "Training model 180 of 180\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_b01_scaledBetween_H1 = test_multiple_models(x_train = np.asarray(X_b01_scaledBetween_H1_train),\n",
    "                                                              y_train = np.asarray(Y_b01_scaledBetween_H1_train),\n",
    "                                                              x_test = np.asarray(X_b01_scaledBetween_H1_test),\n",
    "                                                              y_test = np.asarray(Y_b01_scaledBetween_H1_test),\n",
    "                                                              modelgrid = grid_simple_NN_pl_scaledBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95af87ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_split</th>\n",
       "      <th>epochs</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.33</td>\n",
       "      <td>8</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>8</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>simple_NN_PL_model_2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>simple_NN_PL_model_1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>simple_NN_PL_model_3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  learning_rate val_split epochs  accuracy       TPR  \\\n",
       "0    simple_NN_PL_model_3       0.000010      None      3  0.605634  0.853659   \n",
       "1    simple_NN_PL_model_3       0.001000      None     20  0.605634  0.682927   \n",
       "2    simple_NN_PL_model_2       0.000010      0.33      8  0.591549  0.829268   \n",
       "3    simple_NN_PL_model_1       0.000001      None      5  0.577465  1.000000   \n",
       "4    simple_NN_PL_model_1       0.000001      0.33     50  0.577465  0.731707   \n",
       "..                    ...            ...       ...    ...       ...       ...   \n",
       "175  simple_NN_PL_model_1       0.001000      0.33      8  0.366197  0.048780   \n",
       "176  simple_NN_PL_model_2       0.001000      0.33     10  0.352113  0.048780   \n",
       "177  simple_NN_PL_model_2       0.001000      None      8  0.352113  0.048780   \n",
       "178  simple_NN_PL_model_1       0.000001      0.33      5  0.352113  0.121951   \n",
       "179  simple_NN_PL_model_3       0.001000      0.33      5  0.338028  0.048780   \n",
       "\n",
       "          TNR  \n",
       "0    0.266667  \n",
       "1    0.500000  \n",
       "2    0.266667  \n",
       "3    0.000000  \n",
       "4    0.366667  \n",
       "..        ...  \n",
       "175  0.800000  \n",
       "176  0.766667  \n",
       "177  0.766667  \n",
       "178  0.666667  \n",
       "179  0.733333  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_simple_NN_b01_scaledBetween_H1 = results_simple_NN_b01_scaledBetween_H1.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_b01_scaledBetween_H1.to_csv(folderpath_results + \"results_simple_NN_b01_scaledBetween_H1.csv\",\n",
    "                                              encoding='utf-8',\n",
    "                                              index=False)\n",
    "\n",
    "# Print results\n",
    "results_simple_NN_b01_scaledBetween_H1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725547a",
   "metadata": {},
   "source": [
    "### Train Models on Persistence Landscapes for H1 (scaledWithin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94f4bf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>models_to_combine</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f019c961e50&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;function simple_NN_PL_model_3 at 0x7f019ecfd5e0&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;function simple_NN_PL_model_1 at 0x7f019ecfd700&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;function simple_NN_PL_model_2 at 0x7f019c961e50&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model models_to_combine  \\\n",
       "0  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "1  <function simple_NN_PL_model_2 at 0x7f019c961e50>              None   \n",
       "2  <function simple_NN_PL_model_3 at 0x7f019ecfd5e0>              None   \n",
       "3  <function simple_NN_PL_model_1 at 0x7f019ecfd700>              None   \n",
       "4  <function simple_NN_PL_model_2 at 0x7f019c961e50>              None   \n",
       "\n",
       "  learning_rate validation_split epochs  \n",
       "0          0.01             0.33      3  \n",
       "1          0.01             0.33      3  \n",
       "2          0.01             0.33      3  \n",
       "3         0.001             0.33      3  \n",
       "4         0.001             0.33      3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_simple_NN_pl_scaledWithin = create_modelgrid(models = simple_NN_PL_models,\n",
    "                                                   learning_rates = simple_NN_PL_learning_rates,\n",
    "                                                   validation_splits = simple_NN_PL_validation_splits,\n",
    "                                                   epochs = simple_NN_PL_epochs)\n",
    "grid_simple_NN_pl_scaledWithin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839334e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 180\n",
      "Training model 2 of 180\n",
      "Training model 3 of 180\n",
      "Training model 4 of 180\n",
      "Training model 5 of 180\n",
      "Training model 6 of 180\n",
      "Training model 7 of 180\n",
      "Training model 8 of 180\n",
      "Training model 9 of 180\n",
      "Training model 10 of 180\n",
      "Training model 11 of 180\n",
      "Training model 12 of 180\n",
      "Training model 13 of 180\n",
      "Training model 14 of 180\n",
      "Training model 15 of 180\n",
      "Training model 16 of 180\n",
      "Training model 17 of 180\n",
      "Training model 18 of 180\n",
      "Training model 19 of 180\n",
      "Training model 20 of 180\n",
      "Training model 21 of 180\n",
      "Training model 22 of 180\n",
      "Training model 23 of 180\n",
      "Training model 24 of 180\n",
      "Training model 25 of 180\n",
      "Training model 26 of 180\n",
      "Training model 27 of 180\n",
      "Training model 28 of 180\n",
      "Training model 29 of 180\n",
      "Training model 30 of 180\n",
      "Training model 31 of 180\n",
      "Training model 32 of 180\n",
      "Training model 33 of 180\n",
      "Training model 34 of 180\n",
      "Training model 35 of 180\n",
      "Training model 36 of 180\n",
      "Training model 37 of 180\n",
      "Training model 38 of 180\n",
      "Training model 39 of 180\n",
      "Training model 40 of 180\n",
      "Training model 41 of 180\n",
      "Training model 42 of 180\n",
      "Training model 43 of 180\n",
      "Training model 44 of 180\n",
      "Training model 45 of 180\n",
      "Training model 46 of 180\n",
      "Training model 47 of 180\n",
      "Training model 48 of 180\n",
      "Training model 49 of 180\n",
      "Training model 50 of 180\n",
      "Training model 51 of 180\n",
      "Training model 52 of 180\n",
      "Training model 53 of 180\n",
      "Training model 54 of 180\n",
      "Training model 55 of 180\n",
      "Training model 56 of 180\n",
      "Training model 57 of 180\n",
      "Training model 58 of 180\n",
      "Training model 59 of 180\n",
      "Training model 60 of 180\n",
      "Training model 61 of 180\n",
      "Training model 62 of 180\n",
      "Training model 63 of 180\n",
      "Training model 64 of 180\n",
      "Training model 65 of 180\n",
      "Training model 66 of 180\n",
      "Training model 67 of 180\n",
      "Training model 68 of 180\n",
      "Training model 69 of 180\n",
      "Training model 70 of 180\n"
     ]
    }
   ],
   "source": [
    "results_simple_NN_b01_scaledWithin_H1 = test_multiple_models(x_train = np.asarray(X_b01_scaledWithin_H1_train),\n",
    "                                                              y_train = np.asarray(Y_b01_scaledWithin_H1_train),\n",
    "                                                              x_test = np.asarray(X_b01_scaledWithin_H1_test),\n",
    "                                                              y_test = np.asarray(Y_b01_scaledWithin_H1_test),\n",
    "                                                              modelgrid = grid_simple_NN_pl_scaledWithin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_simple_NN_b01_scaledWithin_H1 = results_simple_NN_b01_scaledWithin_H1.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_b01_scaledWithin_H1.to_csv(folderpath_results + \"results_simple_NN_b01_scaledWithin_H1.csv\",\n",
    "                                              encoding='utf-8',\n",
    "                                              index=False)\n",
    "\n",
    "# Print results\n",
    "results_simple_NN_b01_scaledWithin_H1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf7b49",
   "metadata": {},
   "source": [
    "### Train Models on Persistence Landscapes for H1 (unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e036c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_simple_NN_pl_unscaled = create_modelgrid(models = simple_NN_PL_models,\n",
    "                                              learning_rates = simple_NN_PL_learning_rates,\n",
    "                                              validation_splits = simple_NN_PL_validation_splits,\n",
    "                                              epochs = simple_NN_PL_epochs)\n",
    "grid_simple_NN_pl_unscaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30aacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_simple_NN_b01_unscaled_H1 = test_multiple_models(x_train = np.asarray(X_b01_unscaled_H1_train),\n",
    "                                                             y_train = np.asarray(Y_b01_unscaled_H1_train),\n",
    "                                                             x_test = np.asarray(X_b01_unscaled_H1_test),\n",
    "                                                             y_test = np.asarray(Y_b01_unscaled_H1_test),\n",
    "                                                             modelgrid = grid_simple_NN_pl_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_simple_NN_b01_unscaled_H1 = results_simple_NN_b01_unscaled_H1.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save results to csv\n",
    "results_simple_NN_b01_unscaled_H1.to_csv(folderpath_results + \"results_simple_NN_b01_unscaled_H1.csv\",\n",
    "                                              encoding='utf-8',\n",
    "                                              index=False)\n",
    "\n",
    "results_simple_NN_b01_unscaled_H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b92bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
